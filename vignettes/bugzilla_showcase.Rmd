---
title: "Bugzilla Showcase"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Bugzilla Showcase}
  %\VignetteEncoding{UTF-8}
---
# Introduction
This notebook explains how to fetch bugs from a Bugzilla website by downloading the issues and comments using Perceval and the Bugzilla REST API. We will be using seven functions, which can be found in the R folder.
  ~/kaiaulu/R/download.R: download_bugzilla, download_bugzilla_issues_from_rest_api, download_bugzilla_comments_from_rest_api
  ~/kaiaulu/R/parser.R: parse_bugzilla, parse_bugzillarest, parse_bugzilla_crawler_issue, parse_bugzilla_crawler_comments

Note: Perceval has two endpoints for Bugzilla, so we have two separate functions to parse Bugzilla data from different endpoints. One endpoint is the traditional Bugzilla backend, and the other is the REST API backend for Bugzilla 5.0 (or higher) servers. The REST API backend gives us more information on the issue_creator and issue_assignee than the traditional backend. For more information on Perceval's Bugzilla functions, please check: https://github.com/chaoss/grimoirelab-perceval#bugzilla. To learn more about the Bugzilla REST API, see here: https://wiki.mozilla.org/Bugzilla:REST_API.

At the end of the showcase, we will have the following:
- json objects of Bugzilla data downloaded by Perceval from a Bugzilla website
- json files of Bugzilla data downloaded using the Bugzilla REST API
- data tables with the parsed Bugzilla issues and comments

# Libraries

Please ensure the following R packages are installed on your computer. 

```{r warning = FALSE, message = FALSE}
rm(list = ls())
seed <- 1
set.seed(seed)

require(kaiaulu)
require(stringi)
require(data.table)
require(jsonlite)
```

# Project Configuration File (Parameters Needed)
The parameters necessary for analysis are kept in a project configuration file to ensure reproducibility. In this project, we will use Perceval, the path to which is kept in the `tools.yml` file, to download the Bugzilla data. 
```{r}
tools_path <- "../tools.yml"
tool <- yaml::read_yaml(tools_path)
perceval_path <- tool[["perceval"]]
```

# Bugzilla Wrapper
This section will cover downloading and parsing bugzilla data using Perceval.

## Download Bugzilla data using Perceval's traditional Bugzilla Backend
We start by downloading the issues as json files using Perceval's traditional Bugzilla backend.

Variable definitions:
  datetime: date and time to start bug retrieval
  bugzilla_site: URL to the Bugzilla site
  bugzilla_json: json object downloaded from the Bugzilla site

The bugzilla_json will be used to parse Bugzilla data and create a table of issues.

```{r}
datetime <- "2023-04-16T00:14:57Z"
bugzilla_site <- "https://bugzilla.redhat.com/"
bugzilla_json <- download_bugzilla(perceval_path, bugzilla_site, datetime, backend="bugzilla")
```

## Parse Bugzilla data obtained from Perceval's traditional Bugzilla backend
Next, we will parse the json object we downloaded from the previous step by using parse_bugzilla function.
First, let's try parsing just the issues without the comments.
```{r}
parse_bugzilla(bugzilla_json, comments=FALSE)
```

Next, let's see how the parsed table looks with the comments included.
```{r}
parse_bugzilla(bugzilla_json, comments=TRUE)
```

## Download Bugzilla issues and comments using Perceval's REST API Bugzilla Backend
Similar to downloading Bugzilla data using Perceval's traditional Bugzilla backend, we can also download the Bugzilla json object using Perceval's REST API Bugzilla backend.

Note the explicit use of the `max_bugs` parameter. Max_bugs represents the maximum number of bugs requested on the same query. This acts as the `limit` parameter in the Bugzilla REST API. The `limit` represents how many issues can be pulled at a time and saved into a file, in short, how many issues make up a page. Bugzilla sites may have specific limits set, and Perceval does not account for this implicitly, so make sure to research the Bugzilla site you are using and adjust max_bugs appropriately. For Bugzilla Redhat used below, their limit is set to 20 for unauthenticated users. As such, max_bugs is set to 20.
```{r}
datetime <- "2023-04-16T00:14:57Z"
bugzilla_site <- "https://bugzilla.redhat.com/"
bugzilla_rest_json <- download_bugzilla(perceval_path, bugzilla_site, datetime, backend="bugzillarest", max_bugs=20)
```

## Parse Bugzilla data obtained from Perceval's REST API Bugzilla backend
We can use the 'parse_bugzillarest' function below to get a data table of Bugzilla issues without comments.
```{r}
parse_bugzillarest(bugzilla_rest_json, comments=FALSE)
```

If comments are of interest, we have the option to include these in our parsed Bugzilla data table.
```{r}
parse_bugzillarest(bugzilla_rest_json, comments=TRUE)
```

# Bugzilla crawler
This section will cover downloading and parsing bugzilla data using the Bugzilla REST API. 
Using the Bugzilla REST API directly instead of Perceval's Bugzilla REST API endpoint allows us to bypass the use of third party tools and in turn improve the speed of data retrieval.

## Download Bugzilla issues and comments from REST API
We start by downloading the issues and comments as json files using REST API.

Variable definitions:
  start_timestamp: the date and time to start bug retrieval
  bugzilla_site: URL to the Bugzilla site
  save_issues_path: the folder to save json files containing Bugzilla issues. Each file saved is a page of Bugzilla issue data.
  The name of each file represents the page number.
  save_comments_path: the folder to save json files containing Bugzilla comments. Each file saved contains all the comments for a particular issue. The name of each file represents the issue id that the comments are related to.
  limit_upperbound: the number of issues saved in each page file. Again, some bugzilla sites have limits set on how many bugs
can be retrieved in one GET request, in which case, the limit set by the bugzilla site will be used in place of
limit_upperbound to ensure full bug retrieval. Here, limit_upperbound is set to 20 for the Redhat Bugzilla site, but if it were larger the download_bugzilla_comments_from_rest_api function would be able to account for this.

The save_issues_path and save_comments_path will be used to store Bugzilla data.
```{r}
bugzilla_site <- "https://bugzilla.redhat.com/"
start_timestamp <- "2023-04-16T00:14:57Z"
save_issues_path <- "../../rawdata/bugzilla/issues"
bug_ids <- download_bugzilla_issues_from_rest_api(bugzilla_site, start_timestamp, save_issues_path, limit_upperbound=20)

save_comments_path <- "../../rawdata/bugzilla/comments"
download_bugzilla_comments_from_rest_api(bugzilla_site, bug_ids, save_comments_path)
```

## Parse Bugzilla issues data obtained from REST API
We can use the 'parse_bugzilla_crawler_issue' function below to parse the issues stored in the save_issues_path and retrieve a data table of Bugzilla issues.
```{r}
parse_bugzilla_crawler_issue(save_issues_path)
```

## Parse Bugzilla comments data obtained from REST API
We can use the 'parse_bugzilla_crawler_comments' function below to parse the comments stored in the save_comments_path and retrieve a data table of Bugzilla comments.
```{r}
parse_bugzilla_crawler_comments(save_comments_path)
```
